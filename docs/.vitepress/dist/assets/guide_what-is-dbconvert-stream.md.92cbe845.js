import{_ as a,c as n,b as e,d as r,t as o,a as s,o as i}from"./app.47236b7f.js";const l="/images/dbconvert-stream-high-level-diagram.png",c="/images/log-cdc.png",C=JSON.parse('{"title":"What is DBConvert Stream?","description":"What is DBConvert Stream data replication platform?","frontmatter":{"title":"What is DBConvert Stream?","description":"What is DBConvert Stream data replication platform?","layout":"doc","lastUpdated":true},"headers":[{"level":2,"title":"Collecting data","slug":"collecting-data","link":"#collecting-data","children":[]},{"level":2,"title":"Processing data","slug":"processing-data","link":"#processing-data","children":[{"level":3,"title":"\\"CREATE Table\\" Translation between SQL dialects","slug":"create-table-translation-between-sql-dialects","link":"#create-table-translation-between-sql-dialects","children":[]},{"level":3,"title":"Consistency and Concurrency","slug":"consistency-and-concurrency","link":"#consistency-and-concurrency","children":[]}]},{"level":2,"title":"Delivering data","slug":"delivering-data","link":"#delivering-data","children":[{"level":3,"title":"Horizontal Scaling of services.","slug":"horizontal-scaling-of-services","link":"#horizontal-scaling-of-services","children":[]}]}],"relativePath":"guide/what-is-dbconvert-stream.md","lastUpdated":1663172001000}'),d={name:"guide/what-is-dbconvert-stream.md"},h={id:"frontmatter-title",tabindex:"-1"},p=e("a",{class:"header-anchor",href:"#frontmatter-title","aria-hidden":"true"},"#",-1),u=s('<p><img src="'+l+'" alt="high-level diagram of DBConvert Stream architecture"></p><p>DBConvert Stream is a data integration and streaming distributed platform to replicate data between databases.</p><p>Database connection adapters collect data from and deliver data <strong>continuously</strong> to SQL and no-SQL databases, data warehouses, files on your premises or in the cloud.</p><p>DBConvert Stream platform outputs its internal metrics in Prometheus format to explore and visualize live data in dashboards.</p><h2 id="collecting-data" tabindex="-1">Collecting data <a class="header-anchor" href="#collecting-data" aria-hidden="true">#</a></h2><p>DBConvert Stream&#39;s <em>Database Readers</em> don&#39;t have to wait for a database to completely ingest and index new data before reading it.</p><p>DBConvert Stream uses technology known as Change Data Capture (CDC) to capture row-level stream of <strong>Insert, Update, and Delete Events</strong> from the database transaction log and publishes it to the <strong>Event Bus subsystem</strong>.</p><p><img src="'+c+'" alt="Transaction Log Change Data Capture"></p><p>DBConvert Stream currently supports the following sources:</p><ul><li>MySQL Binlog or MariaDB Binlog</li><li>PostgreSQL WAL or CockroachDB WAL</li></ul><p>Sources are defined and configured via a simple set of properties in <code>JSON</code> format.</p><h2 id="processing-data" tabindex="-1">Processing data <a class="header-anchor" href="#processing-data" aria-hidden="true">#</a></h2><h3 id="create-table-translation-between-sql-dialects" tabindex="-1">&quot;CREATE Table&quot; Translation between SQL dialects <a class="header-anchor" href="#create-table-translation-between-sql-dialects" aria-hidden="true">#</a></h3><p>DBConvert Stream can automatically convert the <code>CREATE Table</code> DDL statements between MySQL and PostgreSQL. Thus, there is no need to worry about manual type conversion of MySQL and Postgres databases. If the destination does not have a corresponded table, the original script will be translated into the appropriate dialect and the new table will be created on the target.</p><h3 id="consistency-and-concurrency" tabindex="-1">Consistency and Concurrency <a class="header-anchor" href="#consistency-and-concurrency" aria-hidden="true">#</a></h3><p>Consistency in distributed system is no longer natural, and horizontal scaling becomes more difficult. DBConvert Stream executes the <code>UPDATE</code> and <code>DELETE</code> SQL statements sequentially in the exact order they arrive from the transaction log.</p><p>Multiple neighbour INSERT statements are executed simultaneously, greatly speeding up the whole process.</p><div class="info custom-block"><p class="custom-block-title">INFO</p><p>Replicating INSERT statements for One Million rows takes about 12 seconds.</p></div><h2 id="delivering-data" tabindex="-1">Delivering data <a class="header-anchor" href="#delivering-data" aria-hidden="true">#</a></h2><p>Along with getting new events from the sources, Event Bus delivers a data stream to all consumers (destinations) simultaneously, subscribed to the current job. DBConvert Stream can write continuously to either MySQL or Postgres target databases.</p><h3 id="horizontal-scaling-of-services" tabindex="-1">Horizontal Scaling of services. <a class="header-anchor" href="#horizontal-scaling-of-services" aria-hidden="true">#</a></h3><p>Running multiple instances of &quot;destination processing service&quot;, improves overall performance in times.</p><p>Destinations are defined and configured via a simple set of properties in <code>JSON</code> format.</p><p>DBConvert Stream has been architected from the ground up to scale.</p>',24);function g(t,m,v,b,f,S){return i(),n("div",null,[e("h1",h,[r(o(t.$frontmatter.title)+" ",1),p]),u])}const D=a(d,[["render",g]]);export{C as __pageData,D as default};
